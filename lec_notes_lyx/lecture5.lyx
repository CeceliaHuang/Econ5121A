#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass report
\use_default_options false
\begin_modules
theorems-ams-chap-bytype
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "palatino" "default"
\font_sans "default" "default"
\font_typewriter "mathpazo" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing double
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref section
\pdf_pdfusetitle false
\pdf_quoted_options "urlcolor=urlcolor,linkcolor=linkcolor,citecolor=citecolor,"
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\bullet 1 0 9 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{chapter}{4}
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Asymptotic Theory
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Our universe, though enormous, consists of fewer than 
\begin_inset Formula $10^{82}$
\end_inset

 atoms, which is a finite number.
 However, mathematical ideas are not bounded by secular realities.
 Asymptotic theory is about behaviors of statistics when the sample size
 is arbitrarily large up to infinity.
 It is a set of approximation techniques to simplify complicated finite-sample
 analysis.
 Asymptotic theory is the cornerstone of modern econometrics.
 It sheds lights on estimation and inference procedures under much more
 general conditions than what exact finite sample theory can accommodate.
\end_layout

\begin_layout Standard
Nevertheless, we always have at hand a finite sample, and mostly it is difficult
 to increase the sample size.
 Asymptotic theory rarely answers 
\begin_inset Quotes eld
\end_inset

how large is large
\begin_inset Quotes erd
\end_inset

, and we must be cautious about the treacherous landscape of asymptopia.
 In the era of big data, albeit the sheer size of data increases dramatically,
 we build more sophisticated models to better capture heterogeneity in the
 data.
 Large sample is a relative notion to the complexity of the model and underlying
 (in)dependence structure of the data.
\end_layout

\begin_layout Standard
Both the classical parametric approach, which is based on hard-to-verify
 parametric assumptions, and the asymptotic approach, which is based on
 imagery infinite sequences, deviate from the reality.
 Which approach is more constructive can only be judged in case by case.
 The prevalence of asymptotic theory is its mathematical amenability and
 generality.
 The law of evolution elevates asymptotic theory to the throne of statistic
 analysis of our time.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
The required mathematical level to work with asymptotic theory, seemingly
 advance though, is lower than that for the finite sample theory.
 The underlying economics theory predicts that for substitute goods, if
 the price of one good is lowered, the demand increases.
 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Modes of Convergence
\end_layout

\begin_layout Standard
Before we talk about convergence of random variables, we first review what
 is convergence for a non-random sequence.
 Let 
\begin_inset Formula $z_{1},z_{2},\ldots$
\end_inset

 be an infinite sequence of non-random variables.
 Convergence of this non-random sequence means that for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, there exists an 
\begin_inset Formula $N\left(\varepsilon\right)$
\end_inset

 such that for all 
\begin_inset Formula $n>N\left(\varepsilon\right)$
\end_inset

, we have 
\begin_inset Formula $\left|z_{n}-z\right|<\varepsilon$
\end_inset

.
 We say 
\begin_inset Formula $z$
\end_inset

 is the limit of 
\begin_inset Formula $z_{n}$
\end_inset

, and write as 
\begin_inset Formula $z_{n}\to z$
\end_inset

.
\end_layout

\begin_layout Standard
Instead of a deterministic sequence, we are interested in the convergence
 of a sequence of random variables.
 Since a random variable is 
\begin_inset Quotes eld
\end_inset

random
\begin_inset Quotes erd
\end_inset

 thanks to the induced probability measure by the measurable function, we
 must be clear what 
\emph on
convergence
\emph default
 means.
 Several modes of convergence are often encountered.
\end_layout

\begin_layout Itemize
Convergence almost surely* 
\end_layout

\begin_layout Itemize

\emph on
Convergence in probability
\emph default
: for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, as 
\begin_inset Formula $n\to\infty$
\end_inset

 the probability 
\begin_inset Formula $P\left(\omega:\left|z_{n}\left(\omega\right)-z\right|<\varepsilon\right)\to1$
\end_inset

 (or equivalently 
\begin_inset Formula $P\left(\omega:\left|z_{n}\left(\omega\right)-z\right|\geq\varepsilon\right)\to0$
\end_inset

.
 Denoted as 
\begin_inset Formula $z_{n}\stackrel{p}{\to}z$
\end_inset

.
 The limit 
\begin_inset Formula $z$
\end_inset

 can be either a random variable or a non-random constant.
 
\end_layout

\begin_layout Itemize

\emph on
Squared-mean convergence
\emph default
: 
\begin_inset Formula $\lim_{n\to\infty}E\left[\left(z_{n}-z\right)^{2}\right]=0.$
\end_inset

 Denoted as 
\begin_inset Formula $z_{n}\stackrel{m.s.}{\to}z$
\end_inset

.
 
\end_layout

\begin_layout Example
\begin_inset Formula $(z_{n})$
\end_inset

 is a sequence of binary random variables: 
\begin_inset Formula $z_{n}=\sqrt{n}$
\end_inset

 with probability 
\begin_inset Formula $1/n$
\end_inset

, and 
\begin_inset Formula $z_{n}=0$
\end_inset

 with probability 
\begin_inset Formula $1-1/n$
\end_inset

.
 Then 
\begin_inset Formula $z_{n}\stackrel{p}{\to}0$
\end_inset

 but 
\begin_inset Formula $z_{n}\stackrel{m.s.}{\nrightarrow}0$
\end_inset

.
 For any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, we have 
\begin_inset Formula $P\left(\omega:\left|z_{n}\left(\omega\right)-0\right|<\varepsilon\right)=P\left(\omega:z_{n}\left(\omega\right)=0\right)=1-1/n\rightarrow1.$
\end_inset

 Thus 
\begin_inset Formula $z_{n}\stackrel{p}{\to}0$
\end_inset

.
 On the other hand, 
\begin_inset Formula $E\left[\left(z_{n}-0\right)^{2}\right]=n\cdot1/n+0\cdot(1-1/n)=1\nrightarrow0.$
\end_inset

 Thus 
\begin_inset Formula $z_{n}\stackrel{m.s.}{\nrightarrow}0$
\end_inset

.
\end_layout

\begin_layout Standard
Convergence in probability does not count what happens on a subset in the
 sample space of small probability.
 Squared-mean convergence deals with the average over the entire probability
 space.
 If a random variable can take a wild value, with small probability though,
 it may blow away the squared-mean convergence.
 On the contrary, such irregularity does not undermine convergence in probabilit
y.
\end_layout

\begin_layout Standard
Both convergence in probability and squared-mean convergence are about convergen
ce of random variables to a target random variable or constant.
 Instead, convergence in distribution is the convergence of CDF, but the
 random variable.
\end_layout

\begin_layout Itemize
Convergence in distribution: 
\begin_inset Formula $x_{n}\stackrel{d}{\to}x$
\end_inset

 if 
\begin_inset Formula $F\left(x_{n}\right)\to F\left(x\right)$
\end_inset

 for each 
\begin_inset Formula $x$
\end_inset

 on which 
\begin_inset Formula $F\left(x\right)$
\end_inset

 is continuous.
\end_layout

\begin_layout Example
Convergence in distribution is about 
\emph on
pointwise
\emph default
 convergence of CDF, not the random variables themselves.Let 
\begin_inset Formula $x\sim N\left(0,1\right)$
\end_inset

.
 If 
\begin_inset Formula $z_{n}=x+1/n$
\end_inset

, then 
\begin_inset Formula $z_{n}\stackrel{p}{\to}x$
\end_inset

 and of course 
\begin_inset Formula $z_{n}\stackrel{d}{\to}x$
\end_inset

.
 However, if 
\begin_inset Formula $z_{n}=-x+1/n$
\end_inset

, or 
\begin_inset Formula $z_{n}=y+1/n$
\end_inset

 where 
\begin_inset Formula $y\sim N\left(0,1\right)$
\end_inset

 is independent of 
\begin_inset Formula $x$
\end_inset

, then 
\begin_inset Formula $z_{n}\stackrel{d}{\to}x$
\end_inset

 but 
\begin_inset Formula $z_{n}\stackrel{p}{\nrightarrow}x$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Example
\begin_inset Formula $(z_{n})$
\end_inset

 is a sequence of binary random variables: 
\begin_inset Formula $z_{n}=\sqrt{n}$
\end_inset

 with probability 
\begin_inset Formula $1/n$
\end_inset

, and 
\begin_inset Formula $z_{n}=0$
\end_inset

 with probability 
\begin_inset Formula $1-1/n$
\end_inset

.
 Then 
\begin_inset Formula $z_{n}\stackrel{d}{\to}z=0.$
\end_inset

 Because 
\begin_inset Formula 
\[
F\left(z_{n}\right)=\begin{cases}
0 & z_{n}<0\\
1-1/n & 0\leq z_{n}\leq\sqrt{n}\\
1 & z_{n}\geq\sqrt{n}
\end{cases}.
\]

\end_inset

 
\begin_inset Formula $F\left(z\right)=\begin{cases}
0 & z<0\\
1 & z\geq0
\end{cases}$
\end_inset

.
 It is obvious that 
\begin_inset Formula $F\left(z_{n}\right)$
\end_inset

 converges to 
\begin_inset Formula $F\left(z\right)$
\end_inset

 pointwise on the set where 
\begin_inset Formula $F\left(z\right)$
\end_inset

 is continuous.
\end_layout

\begin_layout Standard
Squared-mean convergence implies convergence in probability.
 Convergence in probability implies convergence in distribution.
 Cramer-Wold device handles convergence in distribution for random vectors.
 We say a sequence of 
\begin_inset Formula $K$
\end_inset

-dimensional random vectors 
\begin_inset Formula $\left(X_{n}\right)$
\end_inset

 converge in distribution to 
\begin_inset Formula $X$
\end_inset

 if 
\begin_inset Formula $\lambda'X_{n}\stackrel{d}{\to}\lambda'X$
\end_inset

 for any 
\begin_inset Formula $\lambda\in\mathbb{R}^{K}$
\end_inset

.
\end_layout

\begin_layout Subsection
Law of Large Numbers
\end_layout

\begin_layout Standard
(Weak) law of large numbers (LLN) is a collection of statements about convergenc
e in probability of the sample average to its population counterpart.
 The basic form of LLN is: 
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}(z_{i}-E[z_{i}])\stackrel{p}{\to}0
\]

\end_inset

as 
\begin_inset Formula $n\to\infty$
\end_inset

.
 Various versions of LLN work under different assumptions about the distribution
s and dependence of the random variables.
\end_layout

\begin_layout Itemize
Chebyshev LLN: if 
\begin_inset Formula $\left(z_{1},\ldots,z_{n}\right)$
\end_inset

 is a sample of i.i.d.
\begin_inset space ~
\end_inset

observations, 
\begin_inset Formula $E\left[z_{1}\right]=\mu$
\end_inset

 , and 
\begin_inset Formula $\sigma^{2}=\mathrm{var}\left[z_{1}\right]<\infty$
\end_inset

 exists, then 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}z_{i}-\mu\stackrel{p}{\to}0.$
\end_inset

 
\end_layout

\begin_layout Standard
Chebyshev LLN utilizes 
\emph on
Chebyshev inequality
\emph default
.
\end_layout

\begin_layout Itemize

\emph on
Chebyshev inequality
\emph default
: for any random variable 
\begin_inset Formula $x$
\end_inset

 , we have 
\begin_inset Formula $P\left(\left|x\right|>\varepsilon\right)\leq E\left[x^{2}\right]/\varepsilon^{2}$
\end_inset

 for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, if 
\begin_inset Formula $E\left[x^{2}\right]<\infty$
\end_inset

.
 
\end_layout

\begin_layout Standard
Chebyshev inequality is a special case of 
\emph on
Markov inequality
\emph default
.
\end_layout

\begin_layout Itemize

\emph on
Markov inequality
\emph default
: 
\begin_inset Formula $P\left(\left|x\right|>\varepsilon\right)\leq E\left[\left|x\right|^{r}\right]/\varepsilon^{r}$
\end_inset

 for 
\begin_inset Formula $r\geq1$
\end_inset

 and any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, if 
\begin_inset Formula $E\left[\left|x\right|^{r}\right]<\infty$
\end_inset

.
\end_layout

\begin_layout Standard
It is easy to verify Markov inequality.
 
\begin_inset Formula 
\[
\begin{aligned}E\left[\left|x\right|^{r}\right] & =\int_{\left|x\right|>\varepsilon}\left|x\right|^{r}dF_{X}+\int_{\left|x\right|\leq\varepsilon}\left|x\right|^{r}dF_{X}\\
 & \geq\int_{\left|x\right|>\varepsilon}\left|x\right|^{r}dF_{X}\geq\varepsilon^{r}\int_{\left|x\right|>\varepsilon}dF_{X}=\varepsilon^{r}P\left(\left|x\right|>\varepsilon\right).
\end{aligned}
\]

\end_inset


\end_layout

\begin_layout Standard
Consider a partial sum 
\begin_inset Formula $S_{n}=\sum_{i=1}^{n}x_{i}$
\end_inset

, where 
\begin_inset Formula $\mu_{i}=E\left[x_{i}\right]$
\end_inset

 and 
\begin_inset Formula $\sigma_{i}^{2}=\mathrm{var}\left[x_{i}\right]$
\end_inset

.
 We apply the Chebyshev inequality to the sample mean 
\begin_inset Formula $\overline{x}-\bar{\mu}=n^{-1}\left(S_{n}-E\left[S_{n}\right]\right)$
\end_inset

.
 
\begin_inset Formula 
\[
\begin{aligned}P\left(\left|\bar{x}-\bar{\mu}\right|\geq\varepsilon\right) & =P\left(\left|S_{n}-E\left[S_{n}\right]\right|\geq n\varepsilon\right)\\
 & \leq\left(n\varepsilon\right)^{-2}E\left[\sum_{i=1}^{n}\left(x_{i}-\mu_{i}\right)^{2}\right]\\
 & =\left(n\varepsilon\right)^{-2}\mathrm{var}\left(\sum_{i=1}^{n}x_{i}\right)\\
 & =\left(n\varepsilon\right)^{-2}\left[\sum_{i=1}^{n}\mathrm{var}\left(x_{i}\right)+\sum_{i=1}^{n}\sum_{j\neq i}\mathrm{cov}\left(x_{i},x_{j}\right)\right].
\end{aligned}
\]

\end_inset


\end_layout

\begin_layout Standard
From the above derivation, convergence in probability holds as long as the
 right-hand side shrinks to 0 as 
\begin_inset Formula $n\to\infty$
\end_inset

.
 Actually, the convergence can be maintained under much more general conditions
 than just under the i.i.d.
\begin_inset space ~
\end_inset

assumption.
 The random variables in the sample do not have to be identically distributed,
 and they do not have to be independent either.
\end_layout

\begin_layout Standard
Another useful LLN is 
\emph on
Kolmogorov LLN
\emph default
.
 Since its derivation requires advanced knowledge of probability theory,
 we state the result without proof.
\end_layout

\begin_layout Itemize
Kolmogorov LLN: if 
\begin_inset Formula $\left(z_{1},\ldots,z_{n}\right)$
\end_inset

 is a sample of i.i.d.
\begin_inset space ~
\end_inset

observations and 
\begin_inset Formula $E\left[z_{1}\right]=\mu$
\end_inset

 exists, then 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}z_{i}-\mu\stackrel{p}{\to}0.$
\end_inset

 
\end_layout

\begin_layout Standard
Compared to Chebyshev LLN, Kolmogorov LLN only requires the existence of
 the population mean, but not any higher moments.
 On the other hand, i.i.d.
\begin_inset space ~
\end_inset

is essential for Kolmogorov LLN.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

sample.mean = function( n, distribution ){
\end_layout

\begin_layout Plain Layout

# get sample mean for a given distribution
\end_layout

\begin_layout Plain Layout

  if (distribution == "normal"){ y = rnorm( n ) } 
\end_layout

\begin_layout Plain Layout

  else if (distribution == "t2") {y = rt(n, 2) }
\end_layout

\begin_layout Plain Layout

  else if (distribution == "cauchy") {y = rcauchy(n) }
\end_layout

\begin_layout Plain Layout

  return( mean(y) )
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

LLN.plot = function(distribution){
\end_layout

\begin_layout Plain Layout

# draw the sample mean graph
\end_layout

\begin_layout Plain Layout

  ybar = rep(0, length(NN) )
\end_layout

\begin_layout Plain Layout

  for ( i in 1:length(NN)){
\end_layout

\begin_layout Plain Layout

    n = NN[i]; ybar[i] = sample.mean(n, distribution)
\end_layout

\begin_layout Plain Layout

  }  
\end_layout

\begin_layout Plain Layout

  plot(ybar, type = "l", col = "red", ylab = "mean", xlab = "", 
\end_layout

\begin_layout Plain Layout

  lwd = 2, main = distribution)
\end_layout

\begin_layout Plain Layout

  abline(h = 0, lty = 2)
\end_layout

\begin_layout Plain Layout

  return(ybar)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# calculation
\end_layout

\begin_layout Plain Layout

NN = 2^(1:20); set.seed(888); par(mfrow = c(3,1))
\end_layout

\begin_layout Plain Layout

l1 = LLN.plot("normal"); l2 = LLN.plot("t2"); l3 = LLN.plot("cauchy")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Central Limit Theorem
\begin_inset CommandInset label
LatexCommand label
name "central-limit-theorem"

\end_inset


\end_layout

\begin_layout Standard
The central limit theorem (CLT) is a collection of probability results about
 the convergence in distribution to a stable law, usually the normal distributio
n.
 The basic form of the CLT is: for a sample 
\begin_inset Formula $\left(z_{1},\ldots,z_{n}\right)$
\end_inset

 of 
\emph on
zero-mean
\emph default
 random variables, 
\begin_inset Formula 
\begin{equation}
\frac{1}{\sqrt{n}}\sum_{i=1}^{n}z_{i}\stackrel{d}{\to}N\left(0,\sigma^{2}\right).\label{eq:clt}
\end{equation}

\end_inset

Various versions of CLT work under different assumptions about the random
 variables.
\end_layout

\begin_layout Standard

\emph on
Lindeberg-Levy CLT
\emph default
 is the simplest CLT.
\end_layout

\begin_layout Itemize
If the sample is i.i.d., 
\begin_inset Formula $E\left[x_{1}\right]=0$
\end_inset

 and 
\begin_inset Formula $\mathrm{var}\left[x_{1}^{2}\right]=\sigma^{2}<\infty$
\end_inset

, then CLT holds.
 
\end_layout

\begin_layout Standard
Lindeberg-Levy CLT is easy to verify by the characteristic function.
 For any random variable 
\begin_inset Formula $x$
\end_inset

, the function 
\begin_inset Formula $\varphi_{x}\left(t\right)=E\left[\exp\left(ixt\right)\right]$
\end_inset

 is called its 
\emph on
characteristic function
\emph default
.
 The characteristic function fully describes a distribution, just like PDF
 or CDF.
 For example, the characteristic function of 
\begin_inset Formula $N\left(\mu,\sigma^{2}\right)$
\end_inset

 is 
\begin_inset Formula $\exp\left(it\mu-\frac{1}{2}\sigma^{2}t^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Here is a very heuristic argument.
 If 
\begin_inset Formula $E\left[\left|x\right|^{k}\right]<\infty$
\end_inset

 for a positive integer 
\begin_inset Formula $k$
\end_inset

, then 
\begin_inset Formula 
\[
\varphi_{X}\left(t\right)=1+itE\left[X\right]+\frac{\left(it\right)^{2}}{2}E\left[X^{2}\right]+\ldots\frac{\left(it\right)^{k}}{k!}E\left[X^{k}\right]+o\left(t^{k}\right).
\]

\end_inset

Under the assumption of Lindeberg-Levy CLT, 
\begin_inset Formula 
\[
\varphi_{X_{i}/\sqrt{n}}\left(t\right)=1-\frac{t^{2}}{2n}\sigma^{2}+o\left(\frac{t^{2}}{n}\right)
\]

\end_inset

for all 
\begin_inset Formula $i$
\end_inset

, and by independence we have 
\begin_inset Formula 
\[
\begin{aligned}\varphi_{\frac{1}{\sqrt{n}}\sum_{i=1}^{n}x_{i}}\left(t\right) & =\prod_{i=1}^{n}\varphi_{x_{i}/\sqrt{n}}\left(t\right)=\left(1+i\cdot0-\frac{t^{2}}{2n}\sigma^{2}+o\left(\frac{t^{2}}{n}\right)\right)^{n}\\
 & \to\exp\left(-\frac{\sigma^{2}}{2}t^{2}\right),
\end{aligned}
\]

\end_inset

where the limit is exactly the characteristic function of 
\begin_inset Formula $N\left(0,\sigma^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Lindeberg-Feller CLT: i.n.i.d., and 
\emph on
Lindeberg condition
\emph default
: for any fixed 
\begin_inset Formula $\varepsilon>0$
\end_inset

, 
\begin_inset Formula 
\[
\frac{1}{s_{n}^{2}}\sum_{i=1}^{n}E\left[x_{i}^{2}\cdot\boldsymbol{1}\left\{ \left|x_{i}\right|\geq\varepsilon s_{n}\right\} \right]\to0
\]

\end_inset

where 
\begin_inset Formula $s_{n}=\left(\sum_{i=1}^{n}\sigma_{i}^{2}\right)^{1/2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Lyapunov CLT: i.n.i.d.: 
\begin_inset Formula $\max_{i\leq n}E\left[\left|x_{i}\right|^{3}\right]<C<\infty$
\end_inset

.
\end_layout

\begin_layout Standard
This is a simulated example.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

Z_fun = function(n, distribution){
\end_layout

\begin_layout Plain Layout

  if (distribution == "normal"){
\end_layout

\begin_layout Plain Layout

      z = sqrt(n) * mean(rnorm(n))
\end_layout

\begin_layout Plain Layout

	} else if (distribution == "chisq2") {
\end_layout

\begin_layout Plain Layout

      df = 2; 
\end_layout

\begin_layout Plain Layout

      x = rchisq(n,2)
\end_layout

\begin_layout Plain Layout

      z = sqrt(n) * ( mean(x) - df ) / sqrt(2*df)
\end_layout

\begin_layout Plain Layout

      }
\end_layout

\begin_layout Plain Layout

  return (z)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

CLT_plot = function(n, distribution){
\end_layout

\begin_layout Plain Layout

  Rep = 10000
\end_layout

\begin_layout Plain Layout

  ZZ = rep(0, Rep)
\end_layout

\begin_layout Plain Layout

  for (i in 1:Rep) {ZZ[i] = Z_fun(n, distribution)}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  xbase = seq(-4.0, 4.0, length.out = 100)
\end_layout

\begin_layout Plain Layout

  hist( ZZ, breaks = 100, freq = FALSE, 
\end_layout

\begin_layout Plain Layout

    xlim = c( min(xbase), max(xbase) ),
\end_layout

\begin_layout Plain Layout

    main = paste0(
\begin_inset Quotes eld
\end_inset

hist with sample size 
\begin_inset Quotes erd
\end_inset

, n) )
\end_layout

\begin_layout Plain Layout

  lines(x = xbase, y = dnorm(xbase), col = "red")
\end_layout

\begin_layout Plain Layout

  return (ZZ)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

par(mfrow = c(3,1))
\end_layout

\begin_layout Plain Layout

phist = CLT_plot(2, "chisq2")
\end_layout

\begin_layout Plain Layout

phist = CLT_plot(10, "chisq2")
\end_layout

\begin_layout Plain Layout

phist = CLT_plot(100, "chisq2")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Tools for Transformations
\begin_inset CommandInset label
LatexCommand label
name "tools-for-transformations"

\end_inset


\end_layout

\begin_layout Standard
In their original forms, LLN deals with the sample mean, and CLT handles
 the scaled (by 
\begin_inset Formula $\sqrt{n}$
\end_inset

) and/or standardized (by standard deviation) sample mean.
 However, most of the econometric estimators of interest are functions of
 sample means.
 Therefore, we need tools to handle transformations.
\end_layout

\begin_layout Itemize
Small op: 
\begin_inset Formula $x_{n}=o_{p}\left(r_{n}\right)$
\end_inset

 if 
\begin_inset Formula $x_{n}/r_{n}\stackrel{p}{\to}0$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Big Op: 
\begin_inset Formula $x_{n}=O_{p}\left(r_{n}\right)$
\end_inset

 if for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, there exists a 
\begin_inset Formula $c>0$
\end_inset

 such that 
\begin_inset Formula $P\left(\left|x_{n}\right|/r_{n}>c\right)<\varepsilon$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Continuous mapping theorem 1: If 
\begin_inset Formula $x_{n}\stackrel{p}{\to}a$
\end_inset

 and 
\begin_inset Formula $f\left(\cdot\right)$
\end_inset

 is continuous at 
\begin_inset Formula $a$
\end_inset

, then 
\begin_inset Formula $f\left(x_{n}\right)\stackrel{p}{\to}f\left(a\right)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Continuous mapping theorem 2: If 
\begin_inset Formula $x_{n}\stackrel{d}{\to}x$
\end_inset

 and 
\begin_inset Formula $f\left(\cdot\right)$
\end_inset

 is continuous almost surely on the support of 
\begin_inset Formula $x$
\end_inset

, then 
\begin_inset Formula $f\left(x_{n}\right)\stackrel{d}{\to}f\left(x\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Slutsky's Theorem: If 
\begin_inset Formula $x_{n}\stackrel{d}{\to}x$
\end_inset

 and 
\begin_inset Formula $y_{n}\stackrel{p}{\to}a$
\end_inset

, then
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x_{n}+y_{n}\stackrel{d}{\to}x+a$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{n}y_{n}\stackrel{d}{\to}ax$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{n}/y_{n}\stackrel{d}{\to}x/a$
\end_inset

 if 
\begin_inset Formula $a\neq0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Delta method: if 
\begin_inset Formula $\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,\Omega\right)$
\end_inset

, and 
\begin_inset Formula $f\left(\cdot\right)$
\end_inset

 is continuously differentiable at 
\begin_inset Formula $\theta_{0}$
\end_inset

, then 
\begin_inset Formula 
\[
\sqrt{n}\left(f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)\right)\stackrel{d}{\to}N\left(0,\frac{\partial f}{\partial\theta'}\left(\theta_{0}\right)\Omega\left(\frac{\partial f}{\partial\theta}\left(\theta_{0}\right)\right)'\right).
\]

\end_inset


\end_layout

\begin_layout Proof
Proof: take a Taylor expansion of 
\begin_inset Formula $f\left(\widehat{\theta}\right)$
\end_inset

 around 
\begin_inset Formula $f\left(\theta_{0}\right)$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
to be filled.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Asymptotic Properties of OLS
\begin_inset CommandInset label
LatexCommand label
name "asymptotic-properties-of-ols"

\end_inset


\end_layout

\begin_layout Standard
We apply large sample theory to study the OLS estimator 
\begin_inset Formula $\widehat{\beta}=\left(X'X\right)^{-1}X'Y.$
\end_inset


\end_layout

\begin_layout Subsection
Consistency
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "consistency"

\end_inset


\end_layout

\begin_layout Standard
We say 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 is 
\emph on
consistent
\emph default
 if 
\begin_inset Formula $\widehat{\beta}\stackrel{p}{\to}\beta$
\end_inset

 as 
\begin_inset Formula $n\to\infty$
\end_inset

.
 To verify consistency, we write 
\begin_inset Formula 
\begin{equation}
\widehat{\beta}-\beta=\left(X'X\right)^{-1}X'e=\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\frac{1}{n}\sum_{i=1}^{n}x_{i}e_{i}.\label{eq:ols_d}
\end{equation}

\end_inset

The first term 
\begin_inset Formula 
\[
\widehat{Q}=\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\stackrel{p}{\to}Q=E\left[x_{i}x_{i}'\right],
\]

\end_inset

and the second term 
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}x_{i}e_{i}\stackrel{p}{\to}0.
\]

\end_inset

No matter whether 
\begin_inset Formula $\left(y_{i},x_{i}\right)_{i=1}^{n}$
\end_inset

 is an i.i.d., i.n.i.d., or dependent sample, as long as the convergence in probability
 holds for the above two expressions and 
\begin_inset Formula $Q$
\end_inset

 is an invertible matrix, we have 
\begin_inset Formula $\widehat{\beta}-\beta\stackrel{p}{\to}Q^{-1}0=0$
\end_inset

 by the continuous mapping theorem.
 In other words, 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 is a consistent estimator of 
\begin_inset Formula $\beta$
\end_inset

.
\end_layout

\begin_layout Subsection
Asymptotic Normality
\begin_inset CommandInset label
LatexCommand label
name "asymptotic-normality"

\end_inset


\end_layout

\begin_layout Standard
In finite sample, 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 is a random variable.
 We have shown the distribution of 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 under normality in the previous lecture.
 Without the restrictive normality assumption, how can we characterize the
 randomness of the OLS estimator?
\end_layout

\begin_layout Standard
We know from the previous section that 
\begin_inset Formula $\hat{\beta}-\beta\stackrel{p}{\to}0$
\end_inset

 degenerates to a constant.
 To study its distribution, we must scale it up by a proper multiplier so
 that in the limit it neither degenerates nor explodes.
 The suitable scaling factor is 
\begin_inset Formula $\sqrt{n}$
\end_inset

:
\begin_inset Formula 
\[
\sqrt{n}\left(\widehat{\beta}-\beta\right)=\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\frac{1}{\sqrt{n}}\sum_{i=1}^{n}x_{i}e_{i}.
\]

\end_inset

Since 
\begin_inset Formula $E\left[x_{i}e_{i}\right]=0$
\end_inset

, we apply a CLT to obtain 
\begin_inset Formula 
\[
n^{-1/2}\sum_{i=1}^{n}x_{i}e_{i}\stackrel{d}{\to}N\left(0,\Sigma\right)
\]

\end_inset

where 
\begin_inset Formula $\Sigma=E\left[x_{i}x_{i}'e_{i}^{2}\right]$
\end_inset

.
 By the continuous mapping theorem, 
\begin_inset Formula 
\[
\sqrt{n}\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}Q^{-1}\times N\left(0,\Sigma\right)\sim N\left(0,\Omega\right)
\]

\end_inset

where 
\begin_inset Formula $\Omega=Q^{-1}\Sigma Q^{-1}$
\end_inset

 is called the 
\emph on
asymptotic variance
\emph default
.
 This is the 
\emph on
asymptotic normality
\emph default
 of the OLS estimator.
\end_layout

\begin_layout Standard
Up to now we have derived the asymptotic distribution of 
\begin_inset Formula $\widehat{\beta}$
\end_inset

.
 However, to make it feasible, we still have to estimator the asymptotic
 variance 
\begin_inset Formula $\Omega$
\end_inset

.
 If 
\begin_inset Formula $\widehat{\Sigma}$
\end_inset

 is a consistent estimator of 
\begin_inset Formula $\Sigma$
\end_inset

, then 
\begin_inset Formula $\widehat{\Omega}=\widehat{Q}^{-1}\widehat{\Sigma}\widehat{Q}^{-1}$
\end_inset

 is a consistent estimator of 
\begin_inset Formula $\Omega$
\end_inset

.
 (Of course, there are other ways to estimate the asymptotic variance.) A
 feasible version about the distribution of 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 is 
\begin_inset Formula 
\[
\widehat{\Omega}^{-1/2}\sqrt{n}\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}N\left(0,I_{K}\right).
\]

\end_inset


\end_layout

\begin_layout Standard
In the special case of homoskedasticity, 
\begin_inset Formula $\Sigma=E\left[x_{i}x_{i}'e_{i}^{2}\right]=E\left[x_{i}x_{i}'E\left[e_{i}^{2}|X\right]\right]=\sigma^{2}E\left[x_{i}x_{i}'\right]=\sigma^{2}Q$
\end_inset

 so that 
\begin_inset Formula 
\[
n^{-1/2}\sum_{i=1}^{n}x_{i}e_{i}\stackrel{d}{\to}N\left(0,\sigma^{2}Q\right).
\]

\end_inset

Again by the continuous mapping theorem,
\begin_inset Formula 
\[
\sqrt{n}\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}Q^{-1}\times N\left(0,\sigma^{2}Q\right)\sim N\left(0,\sigma^{2}Q^{-1}\right).
\]

\end_inset

We can estimate the unknown parameter 
\begin_inset Formula $\sigma^{2}$
\end_inset

 by either 
\begin_inset Formula $\widehat{\sigma}^{2}=\widehat{e}'\widehat{e}/\left(n-K\right)$
\end_inset

 or 
\begin_inset Formula $\widehat{\sigma}^{2}=\widehat{e}'\widehat{e}/n$
\end_inset

.
\end_layout

\begin_layout Subsection
Estimation of the Variance
\begin_inset CommandInset label
LatexCommand label
name "estimation-of-the-variance"

\end_inset


\end_layout

\begin_layout Standard
We show all elements of 
\begin_inset Formula $\Sigma=E\left[x_{i}x_{i}'e_{i}^{2}\right]$
\end_inset

 is finite.
 That is, 
\begin_inset Formula $\left\Vert \Sigma\right\Vert _{\infty}:=\max_{i,j\leq K}\left|\sigma_{ij}\right|<\infty$
\end_inset

.
 Let 
\begin_inset Formula $z_{i}=x_{i}e_{i}$
\end_inset

, so 
\begin_inset Formula $\Sigma=E\left[z_{i}z_{i}'\right]$
\end_inset

.
 Because of the Cachy-Schwarz inequality, 
\begin_inset Formula 
\[
\left\Vert \Sigma\right\Vert _{\infty}=\max_{k=1,\ldots,K}E\left[z_{ik}^{2}\right].
\]

\end_inset

For each 
\begin_inset Formula $k$
\end_inset

, 
\begin_inset Formula $E\left[z_{ik}^{2}\right]=E\left[x_{ik}^{2}e_{i}^{2}\right]\leq\left(E\left[x_{ik}^{4}\right]E\left[e_{i}^{4}\right]\right)^{1/2}$
\end_inset

.
\end_layout

\begin_layout Standard
For the estimation of variance, if the error is homoskedastic, 
\begin_inset Formula 
\[
\begin{aligned}\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}^{2} & =\frac{1}{n}\sum_{i=1}^{n}\left(e_{i}+x_{i}'\left(\widehat{\beta}-\beta\right)\right)^{2}\\
 & =\frac{1}{n}\sum_{i=1}^{n}e_{i}^{2}+\left(\frac{2}{n}\sum_{i=1}^{n}e_{i}x_{i}\right)'\left(\widehat{\beta}-\beta\right)+\frac{1}{n}\sum_{i=1}^{n}e_{i}^{2}\left(\widehat{\beta}-\beta\right)'x_{i}x_{i}'\left(\widehat{\beta}-\beta\right).
\end{aligned}
\]

\end_inset

The second term 
\begin_inset Formula 
\[
\left(\frac{2}{n}\sum_{i=1}^{n}e_{i}x_{i}\right)'\left(\widehat{\beta}-\beta\right)=o_{p}\left(1\right)o_{p}\left(1\right)=o_{p}\left(1\right).
\]

\end_inset

The third term 
\begin_inset Formula 
\[
\left(\widehat{\beta}-\beta\right)\left(\frac{1}{n}\sum_{i=1}^{n}e_{i}^{2}x_{i}x'_{i}\right)\left(\widehat{\beta}-\beta\right)=o_{p}\left(1\right)O_{p}\left(1\right)o_{p}\left(1\right)=o_{p}\left(1\right).
\]

\end_inset

As 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}^{2}=\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}^{2}+o_{p}\left(1\right)$
\end_inset

 and 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}e_{i}^{2}=\sigma_{e}^{2}+o_{p}\left(1\right)$
\end_inset

, we have 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}^{2}=\sigma_{e}^{2}+o_{p}\left(1\right)$
\end_inset

.
 In other words, 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}^{2}\stackrel{p}{\to}\sigma_{e}^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
For general heteroskedasticity, 
\begin_inset Formula 
\[
\begin{aligned}\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}^{2} & =\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\left(e_{i}+x_{i}'\left(\widehat{\beta}-\beta\right)\right)^{2}\\
 & =\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'e_{i}^{2}+\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}e_{i}x_{i}'\left(\widehat{\beta}-\beta\right)+\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\left(\left(\widehat{\beta}-\beta\right)'x_{i}\right)^{2}.
\end{aligned}
\]

\end_inset

The third term is bounded by 
\begin_inset Formula 
\[
\begin{aligned} & \mbox{trace}\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\left(\left(\widehat{\beta}-\beta\right)'x_{i}\right)^{2}\right)\\
 & \leq K\max_{k}\frac{1}{n}\sum_{i=1}^{n}x_{ik}^{2}\left[\left(\widehat{\beta}-\beta\right)'x_{i}\right]^{2}\\
 & \leq K\left\Vert \widehat{\beta}-\beta\right\Vert _{2}^{2}\max_{k}\frac{1}{n}\sum_{i=1}^{n}x_{ik}^{2}\left\Vert x_{i}\right\Vert _{2}^{2}\\
 & \leq K\left\Vert \widehat{\beta}-\beta\right\Vert _{2}^{2}\frac{1}{n}\sum_{i=1}^{n}\left\Vert x_{i}\right\Vert _{2}^{2}\left\Vert x_{i}\right\Vert _{2}^{2}\\
 & =K\left\Vert \widehat{\beta}-\beta\right\Vert _{2}^{2}\frac{1}{n}\sum_{i=1}^{n}\left(\sum_{k=1}^{K}x_{ik}^{2}\right)^{2}\\
 & \leq K\left\Vert \widehat{\beta}-\beta\right\Vert _{2}^{2}K\sum_{k=1}^{K}\frac{1}{n}\sum_{i=1}^{n}x_{ik}^{4}=o_{p}\left(1\right)O_{p}\left(1\right)=o_{p}\left(1\right).
\end{aligned}
\]

\end_inset

where the third inequality follows by 
\begin_inset Formula $\left(a_{1}+\cdots+a_{K}\right)^{2}\leq K\left(a_{1}^{2}+\cdots+a_{K}^{2}\right)$
\end_inset

.
 The second term is bounded by 
\begin_inset Formula 
\[
\begin{aligned} & \left|\frac{1}{n}\sum_{i=1}^{n}x_{ik}x_{ik'}e_{i}x_{i}'\left(\widehat{\beta}-\beta\right)\right|\\
 & \leq\max_{k}\left|\widehat{\beta}_{k}-\beta_{k}\right|K\max_{k,k',k''}\left|\frac{1}{n}\sum_{i=1}^{n}e_{i}x_{ik}x_{ik'}x_{ik''}\right|\\
 & \leq\left\Vert \widehat{\beta}-\beta\right\Vert _{2}\left(\frac{1}{n}\sum_{i=1}^{n}e_{i}^{4}\right)^{1/4}K\max_{k,k',k''}\left(\frac{1}{n}\sum_{i=1}^{n}\left(x_{ik}x_{ik'}x_{ik''}\right)^{4/3}\right)^{3/4}\\
 & \leq\left\Vert \widehat{\beta}-\beta\right\Vert _{2}K\max_{k}\left(\frac{1}{n}\sum_{i=1}^{n}x_{ik}^{4}\right)^{3/4}=o_{p}\left(1\right)O_{p}\left(1\right)
\end{aligned}
\]

\end_inset

where the second and the third inequality hold by the Holder's inequality.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bigskip
\end_layout

\begin_layout Plain Layout


\backslash
texttt{ Zhentao Shi.
 
\backslash
today}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
