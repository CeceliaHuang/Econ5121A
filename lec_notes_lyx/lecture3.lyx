#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass report
\begin_preamble
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams-chap-bytype
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "palatino" "default"
\font_sans "default" "default"
\font_typewriter "mathpazo" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing double
\use_hyperref false
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref section
\pdf_pdfusetitle false
\pdf_quoted_options "urlcolor=urlcolor,linkcolor=linkcolor,citecolor=citecolor,"
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{chapter}{2}
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Least Squares
\end_layout

\begin_layout Standard
Notation: 
\begin_inset Formula $y_{i}$
\end_inset

 is a scalar, and 
\begin_inset Formula $x_{i}=\left(x_{i1},\ldots,x_{iK}\right)'$
\end_inset

 is a 
\begin_inset Formula $K\times1$
\end_inset

 vector.
 
\begin_inset Formula $Y=\left(y_{1},\ldots,y_{n}\right)'$
\end_inset

 is an 
\begin_inset Formula $n\times1$
\end_inset

 vector, and 
\begin_inset Formula 
\[
X=\left[\begin{array}{c}
x_{1}'\\
x_{2}'\\
\vdots\\
x_{n}'
\end{array}\right]=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1K}\\
x_{21} & x_{22} & \cdots & x_{2K}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{22} & \cdots & x_{nK}
\end{array}\right]
\]

\end_inset

 is an 
\begin_inset Formula $n\times K$
\end_inset

 matrix.
 
\begin_inset Formula $I_{n}$
\end_inset

 is an 
\begin_inset Formula $n\times n$
\end_inset

 identity matrix.
\end_layout

\begin_layout Standard
Ordinary least squares (OLS) is the most basic estimation technique in econometr
ics.
 It is simple and transparent.
 Understanding it thoroughly paves the way to study more sophisticated linear
 estimators.
 Moreover, many nonlinear estimators resemble the behavior of linear estimators
 in a neighborhood of the true value.
 
\end_layout

\begin_layout Standard
In this lecture, we study the finite sample properties of OLS.
 We first learn a series of facts from the linear algebra operation.
 Next, noticing that OLS coincides with the maximum likelihood estimator
 if the error term follows a normal distribution, we derive its finite-sample
 exact distribution which can be used for statistical inference.
 Finally, the Gauss-Markov theorem justifies the optimality of OLS under
 the classical assumptions.
\end_layout

\begin_layout Standard
To manipulate Leopold Kronecker's famous saying 
\begin_inset Quotes eld
\end_inset

God made the integers; all else is the works of man
\begin_inset Quotes erd
\end_inset

, I would say 
\begin_inset Quotes eld
\end_inset

Gauss made OLS; all else is the works of applied researchers.
\begin_inset Quotes erd
\end_inset

 Popularity of OLS goes far beyond our dismal science.
 But be aware that OLS is a pure statistical or supervised machine learning
 method which reveals correlation instead of causality.
 Rather, economic theory hypothesizes causality while data are collected
 to test the theory or quantify the effect.
  
\begin_inset Note Note
status open

\begin_layout Plain Layout
According to Karl Popper, empirical evidence can only falsify a hypothesis
 but can never verify a hypothesis.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Algebra of Least Squares 
\begin_inset CommandInset label
LatexCommand label
name "algebra-of-least-squares"

\end_inset


\end_layout

\begin_layout Subsection
OLS 
\begin_inset CommandInset label
LatexCommand label
name "ols-estimator"

\end_inset


\end_layout

\begin_layout Standard
As we have learned from the linear project model, the projection coefficient
 
\begin_inset Formula $\beta$
\end_inset

 in the regression 
\begin_inset Formula 
\[
\begin{aligned}y & =x'\beta+e\end{aligned}
\]

\end_inset

can be written as 
\begin_inset Formula 
\begin{equation}
\beta=\left(E\left[xx'\right]\right)^{-1}E\left[xy\right].\label{eq:pop_OLS}
\end{equation}

\end_inset

 We draw a pair of 
\begin_inset Formula $\left(y,x\right)$
\end_inset

 from the joint distribution, and we mark it as 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 for 
\begin_inset Formula $i=1,\ldots,n$
\end_inset

 repeated experiments.
 We possess a 
\emph on
sample
\emph default
 
\begin_inset Formula $\left(y_{i},x_{i}\right)_{i=1}^{n}$
\end_inset

.
\end_layout

\begin_layout Remark
Is 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 random or deterministic? Before we make the observation, they are treated
 as random variables whose realized values are uncertain.
 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 is treated as random when we talk about statistical properties â€” statistical
 properties of a fixed number is meaningless.
 After we make the observation, they become deterministic values which cannot
 vary anymore.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
In reality, we have at hand fixed numbers (more recently, words, photos,
 audio clips, video clips, etc., which can all be represented in digital
 formats with 0 and 1) to feed into a computational operation, and the operation
 will return one or some numbers.
 All statistical interpretation about these numbers are drawn from the probabili
stic thought experiments.
 A 
\emph on
thought experiment
\emph default
 is an academic jargon for a 
\emph on
story
\emph default
 in plain language.
 Under the axiomatic approach of probability theory, such stories are mathematic
al consistent and coherent.
 But mathematics is a tautological system, not science.
 The scientific value of a probability model depends on how close it is
 to the 
\emph on
truth
\emph default
 or implications of the truth.
 In this course, we suppose that the data are generated from some mechanism,
 which is taken as the truth.
 In the linear regression model for example, the joint distribution of 
\begin_inset Formula $\left(y,x\right)$
\end_inset

 is the truth, while we are interested in the linear projection coefficient
 
\begin_inset Formula $\beta$
\end_inset

, which is an implication of the truth as in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pop_OLS"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Probabilists suppose there is a dragon and try to tell the dragon's behaviors.
 Statisticians observe many snakes on earth, and try to tell what a dragon
 looks like.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The sample mean is a natural estimator of the population mean.
 Replace the population mean 
\begin_inset Formula $E\left[\cdot\right]$
\end_inset

 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pop_OLS"
plural "false"
caps "false"
noprefix "false"

\end_inset

) by the sample mean 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\cdot$
\end_inset

, and the resulting estimator is 
\begin_inset Formula 
\begin{align*}
\widehat{\beta} & =\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}\\
 & =\left(\frac{X'X}{n}\right)^{-1}\frac{X'y}{n}=\left(X'X\right)^{-1}X'y
\end{align*}

\end_inset

if 
\begin_inset Formula $X'X$
\end_inset

 is invertible.
 This is one way to motivate the OLS estimator.
 
\end_layout

\begin_layout Standard
Alternatively, we can derive the OLS estimator from minimizing the sum of
 squared residuals 
\begin_inset Formula $\sum_{i=1}^{n}\left(y_{i}-x_{i}'b\right)^{2}$
\end_inset

, or equivalently 
\begin_inset Formula 
\[
Q\left(b\right)=\frac{1}{2n}\sum_{i=1}^{n}\left(y_{i}-x_{i}'b\right)^{2}=\frac{1}{2n}\left(Y-Xb\right)'\left(Y-Xb\right)=\frac{1}{2n}\left\Vert Y-Xb\right\Vert ^{2},
\]

\end_inset

where the factor 
\begin_inset Formula $\frac{1}{2n}$
\end_inset

 is nonrandom and does not change the minimizer, and 
\begin_inset Formula $\left\Vert \cdot\right\Vert $
\end_inset

 is the Euclidean norm of a vector.
 Solve the first-order condition 
\begin_inset Formula 
\[
\frac{\partial}{\partial b}Q\left(b\right)=\left[\begin{array}{c}
\partial Q\left(b\right)/\partial b_{1}\\
\partial Q\left(b\right)/\partial b_{2}\\
\vdots\\
\partial Q\left(b\right)/\partial b_{K}
\end{array}\right]=-\frac{1}{n}X'\left(Y-Xb\right)=0.
\]

\end_inset

This necessary condition for optimality gives exactly the same 
\begin_inset Formula $\widehat{\beta}=\left(X'X\right)^{-1}X'y$
\end_inset

.
 Moreover, the second-order condition 
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial b\partial b'}Q\left(b\right)=\left[\begin{array}{cccc}
\frac{\partial^{2}}{\partial b_{1}^{2}}Q\left(b\right) & \frac{\partial^{2}}{\partial b_{2}\partial b_{2}}Q\left(b\right) & \cdots & \frac{\partial^{2}}{\partial b_{K}\partial b_{1}}Q\left(b\right)\\
\frac{\partial^{2}}{\partial b_{1}\partial b_{2}}Q\left(b\right) & \frac{\partial^{2}}{\partial b_{2}^{2}}Q\left(b\right) & \cdots & \frac{\partial^{2}}{\partial b_{K}\partial b_{2}}Q\left(b\right)\\
\vdots & \vdots & \ddots & \vdots\\
\frac{\partial^{2}}{\partial b_{1}\partial b_{K}}Q\left(b\right) & \frac{\partial^{2}}{\partial b_{2}\partial b_{K}}Q\left(b\right) & \cdots & \frac{\partial^{2}}{\partial b_{K}^{2}}Q\left(b\right)
\end{array}\right]=\frac{1}{n}X'X
\]

\end_inset

shows that 
\begin_inset Formula $Q\left(b\right)$
\end_inset

 is convex in 
\begin_inset Formula $b$
\end_inset

 due to the positive semi-definite matrix 
\begin_inset Formula $X'X/n$
\end_inset

.
 (The function 
\begin_inset Formula $Q\left(b\right)$
\end_inset

 is strictly convex in 
\begin_inset Formula $b$
\end_inset

 if 
\begin_inset Formula $X'X/n$
\end_inset

 is positive definite.)
\end_layout

\begin_layout Remark
In the derivation of OLS we presume that the 
\begin_inset Formula $K$
\end_inset

 columns in 
\begin_inset Formula $X$
\end_inset

 are 
\emph on
linearly independent
\emph default
, which means there is no 
\begin_inset Formula $K\times1$
\end_inset

 vector 
\begin_inset Formula $b$
\end_inset

 such that 
\begin_inset Formula $b\neq0_{K}$
\end_inset

 and 
\begin_inset Formula $Xb=0_{n}$
\end_inset

.
 Linear independence of the columns implies 
\begin_inset Formula $n\geq K$
\end_inset

 and the invertibility of 
\begin_inset Formula $X'X/n$
\end_inset

.
 Linear independence is violated when some regressors are 
\emph on
perfectly collinear
\emph default
, for example when we use dummy variables to indicate categorical variables
 and put all these categories into the regression.
 Modern econometrics software automatically detects and reports perfect
 collinearity.
 What is treacherous is 
\emph on
nearly collinear
\emph default
, meaning that the minimal eigenvalue of 
\begin_inset Formula $X'X/n$
\end_inset

 is close to 0, though not exactly equal to 0.
 We will talk about the consequence of near collinearity in the chapter
 of asymptotic theory.
\end_layout

\begin_layout Standard
Here are some definitions and properties of the OLS estimator.
\end_layout

\begin_layout Itemize
Fitted value: 
\begin_inset Formula $\widehat{Y}=X\widehat{\beta}$
\end_inset

.
\end_layout

\begin_layout Itemize
Projection matrix: 
\begin_inset Formula $P_{X}=X\left(X'X\right)^{-1}X$
\end_inset

; Residual maker matrix: 
\begin_inset Formula $M_{X}=I_{n}-P_{X}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $P_{X}X=X$
\end_inset

; 
\begin_inset Formula $X'P_{X}=X'$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $M_{X}X=0_{n\times K}$
\end_inset

; 
\begin_inset Formula $X'M_{X}=0_{K\times n}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $P_{X}M_{X}=M_{X}P_{X}=0_{n\times n}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $AA=A$
\end_inset

, we call it an 
\emph on
idempotent
\emph default
 matrix.
 Both 
\begin_inset Formula $P_{X}$
\end_inset

 and 
\begin_inset Formula $M_{X}$
\end_inset

 are idempotent.
 All eigenvalues of an idempotent matrix must be either 1 or 0.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathrm{rank}\left(P_{X}\right)=K$
\end_inset

, and 
\begin_inset Formula $\mathrm{rank}\left(M_{X}\right)=n-K$
\end_inset

 (See the Appendix of this chapter).
\end_layout

\begin_layout Itemize
Residual: 
\begin_inset Formula $\widehat{e}=Y-\widehat{Y}=Y-X\widehat{\beta}=Y-X(X'X)^{-1}X'Y=(I_{n}-P_{X})Y=M_{X}Y=M_{X}\left(X\beta+e\right)=M_{X}e$
\end_inset

.
 Notice 
\begin_inset Formula $\widehat{e}$
\end_inset

 and 
\begin_inset Formula $e$
\end_inset

 are two different objects.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $X'\widehat{e}=X'M_{X}e=0_{K}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\sum_{i=1}^{n}\widehat{e}_{i}=0$
\end_inset

 if 
\begin_inset Formula $x_{i}$
\end_inset

 contains a constant.
 
\end_layout

\begin_deeper
\begin_layout Standard
(Because 
\begin_inset Formula $X'\widehat{e}=\left[\begin{array}{cccc}
1 & 1 & \cdots & 1\\
\heartsuit & \heartsuit & \cdots & \heartsuit\\
\cdots & \cdots & \ddots & \vdots\\
\heartsuit & \heartsuit & \cdots & \heartsuit
\end{array}\right]\left[\begin{array}{c}
\widehat{e}_{1}\\
\widehat{e}_{2}\\
\vdots\\
\widehat{e}_{n}
\end{array}\right]=\left[\begin{array}{c}
0\\
0\\
\vdots\\
0
\end{array}\right]$
\end_inset

 and the the first row implies 
\begin_inset Formula $\sum_{i=1}^{n}\widehat{e}_{i}=0$
\end_inset

.
 
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $\heartsuit$
\end_inset


\begin_inset Quotes erd
\end_inset

 indicates the entries irrelevant to our purpose.)
\end_layout

\end_deeper
\begin_layout Standard
The operation of OLS bears a natural geometric interpretation.
 Notice 
\begin_inset Formula $\mathcal{X}=\left\{ Xb:b\in\mathbb{R}^{K}\right\} $
\end_inset

 is the linear space spanned by the 
\begin_inset Formula $K$
\end_inset

 columns of 
\begin_inset Formula $X=\left[X_{\cdot1},\ldots,X_{\cdot K}\right]$
\end_inset

, which is of 
\begin_inset Formula $K$
\end_inset

-dimension if the columns are linearly independent.
 The OLS estimator is the minimizer of 
\begin_inset Formula $\min_{b\in\mathbb{R}^{K}}\left\Vert Y-Xb\right\Vert $
\end_inset

 (Square the Euclidean norm or not does not change the minimizer because
 
\begin_inset Formula $a^{2}$
\end_inset

 is a monotonic transformation for 
\begin_inset Formula $a\geq0$
\end_inset

).
 In other words, 
\begin_inset Formula $X\widehat{\beta}$
\end_inset

 is the point in 
\begin_inset Formula $\mathcal{X}$
\end_inset

 such that it is the closest to the vector 
\begin_inset Formula $Y$
\end_inset

 in terms of the Euclidean norm.
\end_layout

\begin_layout Standard
The relationship 
\begin_inset Formula $Y=X\widehat{\beta}+\widehat{e}$
\end_inset

 decomposes 
\begin_inset Formula $Y$
\end_inset

 into two orthogonal vectors 
\begin_inset Formula $X\widehat{\beta}$
\end_inset

 and 
\begin_inset Formula $\widehat{e}$
\end_inset

 as 
\begin_inset Formula $\left\langle X\widehat{\beta},\widehat{e}\right\rangle =\widehat{\beta}X'\widehat{e}=0_{K}^{\prime}$
\end_inset

, where 
\begin_inset Formula $\left\langle \cdot,\cdot\right\rangle $
\end_inset

 is the 
\emph on
inner product
\emph default
 of two vectors.
 Therefore 
\begin_inset Formula $X\widehat{\beta}$
\end_inset

 is the 
\emph on
projection
\emph default
 of 
\begin_inset Formula $Y$
\end_inset

 onto 
\begin_inset Formula $\mathcal{X}$
\end_inset

, and 
\begin_inset Formula $\widehat{e}$
\end_inset

 is the corresponding 
\emph on
projection residuals.
 
\emph default
The Pythagorean theorem implies
\begin_inset Formula 
\[
\left\Vert Y\right\Vert ^{2}=\Vert X\widehat{\beta}\Vert^{2}+\left\Vert \widehat{e}\right\Vert ^{2}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
definecolor{dtsfsf}{rgb}{0.8274509803921568,0.1843137254901961,0.1843137254901961}
 
\backslash
definecolor{wrwrwr}{rgb}{0.3803921568627451,0.3803921568627451,0.3803921568627451}
 
\backslash
definecolor{rvwvcq}{rgb}{0.08235294117647059,0.396078431372549,0.7529411764705882}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1cm,y=1cm,
 scale=0.5] 
\end_layout

\begin_layout Plain Layout


\backslash
clip(-20.36,-10.76) rectangle (3.16,11.16); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-14.94,-1.92) -- (-5.88,-1.8); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-14.94,-1.92) -- (-5.86,5.1); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-5.88,-1.8) -- (-5.86,5.1); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-5.86,5.1) -- (-5.86,5.1); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-5.86,5.1) -- (-5.86,5.1); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-5.86,5.1) -- (-5.86,5.1); 
\backslash
draw [->,line width=2pt,color=wrwrwr] (-5.86,5.1) -- (-5.86,5.1); 
\backslash
draw (-10.26,-2) node[anchor=north west] {$
\backslash
hat{Y} = X 
\backslash
hat{
\backslash
beta}$}; 
\backslash
draw (-10.56,2.94) node[anchor=north west] {$Y$}; 
\backslash
draw (-5.4,2.04) node[anchor=north west] {$
\backslash
hat{e}$}; 
\backslash
draw (-10.12,-3.12) node[anchor=north west] {projection}; 
\backslash
draw (-5.42,1.14) node[anchor=north west] {residual}; 
\backslash
draw [line width=2pt,color=dtsfsf,domain=-20.36:3.16] plot(
\backslash
x,{(-15.6024--0.12*
\backslash
x)/9.06}); 
\backslash
draw (-4.48,-1.72) node[anchor=north west] {$
\backslash
mathcal{X}$}; 
\backslash
begin{scriptsize} 
\backslash
draw [fill=rvwvcq] (-14.94,-1.92) circle (2.5pt); 
\backslash
draw [fill=rvwvcq] (-5.88,-1.8) circle (2.5pt); 
\backslash
draw [fill=rvwvcq] (-5.86,5.1) circle (2.5pt); 
\backslash
draw [fill=rvwvcq] (-5.86,5.1) circle (2.5pt); 
\backslash
end{scriptsize} 
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\end_inset


\end_layout

\begin_layout Example
Here is a simple simulated example to demonstrate the properties of OLS.
 Given 
\begin_inset Formula $\left(x_{1i},x_{2i},x_{3i},e_{i}\right)^{\prime}\sim N\left(0_{4},I_{4}\right)$
\end_inset

, the dependent variable 
\begin_inset Formula $y_{i}$
\end_inset

 is generated from 
\begin_inset Formula 
\[
y_{i}=0.5+2\cdot x_{1i}-1\cdot x_{2i}+e_{i}
\]

\end_inset

The researcher does not know 
\begin_inset Formula $x_{3i}$
\end_inset

 is redundant, and he regresses 
\begin_inset Formula $y_{i}$
\end_inset

 on 
\begin_inset Formula $\left(1,x_{1i},x_{2i},x_{3i}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

library(magrittr); set.seed(2020-9-23)
\end_layout

\begin_layout Plain Layout

n = 20 # sample size  
\end_layout

\begin_layout Plain Layout

K = 4  # number of paramters
\end_layout

\begin_layout Plain Layout

b0 = as.matrix( c(0.5, 2, -1, 0) ) # the true coefficient
\end_layout

\begin_layout Plain Layout

X = cbind(1, matrix( rnorm(n * (K-1)), nrow = n ) )  # the regressor matrix
 
\end_layout

\begin_layout Plain Layout

e = rnorm(n) # the error term
\end_layout

\begin_layout Plain Layout

Y = X %*% b0 + e # generate the dependent variable
\end_layout

\begin_layout Plain Layout

bhat = solve(t(X) %*% X, t(X) %*% Y ) %>% as.vector() %>% print()
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The estimated coefficient 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 is (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{ format(bhat, digits=3) }
\end_layout

\end_inset

).
 It is close to the true value, but not very accurate due to the small sample
 size.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

ehat = Y - X %*% bhat 
\end_layout

\begin_layout Plain Layout

as.vector( t(X) %*% ehat ) %>% print()
\end_layout

\begin_layout Plain Layout

MX = diag(n) - X %*% solve( crossprod(X) ) %*% t(X)
\end_layout

\begin_layout Plain Layout

data.frame(e = e, ehat = ehat, MXY = MX%*%Y, MXe = MX%*%e ) %>% head()
\end_layout

\begin_layout Plain Layout

cat("The mean of the residual is ", mean(ehat), ".
\backslash
n
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Plain Layout

cat(
\begin_inset Quotes eld
\end_inset

The mean of the true error term is", mean(e), 
\begin_inset Quotes eld
\end_inset

.
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Frish-Waugh-Lovell Theorem
\begin_inset CommandInset label
LatexCommand label
name "frish-waugh-lovell-theorem"

\end_inset


\end_layout

\begin_layout Standard
The Frish-Waugh-Lovell (FWL) theorem is an algebraic fact about the formula
 of a subvector of the OLS estimator.
 To derive the FWL theorem we need to use the inverse of partitioned matrix.
 For a positive definite symmetric matrix 
\begin_inset Formula $A=\begin{pmatrix}A_{11} & A_{12}\\
A_{12}' & A_{22}
\end{pmatrix}$
\end_inset

, the inverse can be written as 
\begin_inset Formula 
\[
A^{-1}=\begin{pmatrix}\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1} & -\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1}A_{12}A_{22}^{-1}\\
-A_{22}^{-1}A_{12}'\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1} & \left(A_{22}-A_{12}'A_{11}^{-1}A_{12}\right)^{-1}
\end{pmatrix}.
\]

\end_inset

In our context of OLS estimator, let 
\begin_inset Formula $X=\left(\begin{array}{cc}
X_{1} & X_{2}\end{array}\right)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\begin{pmatrix}\widehat{\beta}_{1}\\
\widehat{\beta}_{2}
\end{pmatrix} & =\widehat{\beta}=(X'X)^{-1}X'Y\\
 & =\left(\begin{pmatrix}X_{1}'\\
X_{2}'
\end{pmatrix}\begin{pmatrix}X_{1} & X_{2}\end{pmatrix}\right)^{-1}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}\\
 & =\begin{pmatrix}X_{1}'X_{1} & X_{1}'X_{2}\\
X_{2}'X_{1} & X_{2}'X_{2}
\end{pmatrix}^{-1}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}\\
 & =\begin{pmatrix}\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1} & -\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'X_{2}\left(X_{2}'X_{2}\right)^{-1}\\
\heartsuit & \heartsuit
\end{pmatrix}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The subvector
\begin_inset Formula 
\begin{align*}
\widehat{\beta}_{1} & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'Y-\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'X_{2}\left(X_{2}'X_{2}\right)^{-1}X_{2}'Y\\
 & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'Y-\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'P_{X_{2}}Y\\
 & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}\left(X_{1}'Y-X_{1}'P_{X_{2}}Y\right)\\
 & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'M_{X_{2}}Y.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Notice that 
\begin_inset Formula $\widehat{\beta}_{1}$
\end_inset

 can be obtained by the following:
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $Y$
\end_inset

 on 
\begin_inset Formula $X_{2}$
\end_inset

, obtain the residual 
\begin_inset Formula $\tilde{Y}$
\end_inset

;
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $X_{1}$
\end_inset

 on 
\begin_inset Formula $X_{2}$
\end_inset

, obtain the residual 
\begin_inset Formula $\tilde{X}_{1}$
\end_inset

;
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $\tilde{Y}$
\end_inset

 on 
\begin_inset Formula $\tilde{X}_{1}$
\end_inset

, obtain OLS estimates 
\begin_inset Formula $\widehat{\beta}_{1}$
\end_inset

.
\end_layout

\begin_layout Standard
Similar derivation can also be carried out in the population linear projection.
 See Hansen (2020) [E] Chapter 2.22-23.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

X1 = X[,1:2];X2 = X[,3:4]
\end_layout

\begin_layout Plain Layout

PX2 = X2 %*% solve( t(X2) %*% X2) %*% t(X2) 
\end_layout

\begin_layout Plain Layout

MX2 = diag(rep(1,n)) - PX2
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

bhat1 <- (solve(t(X1)%*% MX2 %*% X1, t(X1) %*% MX2 %*% Y )) %>%
\end_layout

\begin_layout Plain Layout

  as.vector() %>% print()
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

ehat1 = MX2 %*% Y - MX2 %*% X1 %*% bhat1  
\end_layout

\begin_layout Plain Layout

data.frame(ehat = ehat, ehat1 = ehat1) %>% head() %>% print()
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Goodness of Fit 
\begin_inset CommandInset label
LatexCommand label
name "goodness-of-fit"

\end_inset


\end_layout

\begin_layout Standard
Consider the regression with the intercept 
\begin_inset Formula $Y=X_{1}\beta_{1}+\beta_{2}+e.$
\end_inset

 The OLS estimator gives 
\begin_inset Formula 
\begin{equation}
Y=\widehat{Y}+\widehat{e}=\left(X_{1}\widehat{\beta}_{1}+\widehat{\beta}_{2}\right)+\widehat{e}.\label{eq:decomp_1}
\end{equation}

\end_inset

Applying the FWL theorem with 
\begin_inset Formula $X_{2}=\iota$
\end_inset

, where 
\begin_inset Formula $\iota$
\end_inset

 (Greek letter, iota) is an 
\begin_inset Formula $n\times1$
\end_inset

 vector of 1's.
 Then 
\begin_inset Formula $M_{X_{2}}=M_{\iota}=I_{n}-\frac{1}{n}\iota\iota'$
\end_inset

.
 Notice 
\begin_inset Formula $M_{\iota}$
\end_inset

 is the 
\emph on
demeaner
\emph default
 in that 
\begin_inset Formula $M_{\iota}z=z-\bar{z}$
\end_inset

.
 It subtract the vector mean 
\begin_inset Formula $\bar{z}=\frac{1}{n}\sum_{i=1}^{n}z_{i}$
\end_inset

 from the original vector 
\begin_inset Formula $z$
\end_inset

.
 The above three-step procedure becomes
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $Y$
\end_inset

 on 
\begin_inset Formula $\iota$
\end_inset

, and the residual is 
\begin_inset Formula $M_{\iota}Y$
\end_inset

;
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $X_{1}$
\end_inset

 on 
\begin_inset Formula $\iota$
\end_inset

, and the residual is 
\begin_inset Formula $M_{\iota}X_{1}$
\end_inset

;
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $M_{\iota}Y$
\end_inset

 on 
\begin_inset Formula $M_{\iota}X_{1}$
\end_inset

, and the OLS estimates is exactly the same as 
\begin_inset Formula $\widehat{\beta}_{1}$
\end_inset

 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:decomp_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard
The last step gives the decomposition
\begin_inset Formula 
\begin{equation}
M_{\iota}Y=M_{\iota}X_{1}\widehat{\beta}_{1}+\tilde{e},\label{eq:decomp_2}
\end{equation}

\end_inset

and the Pythagorean theorem implies
\begin_inset Formula 
\[
\left\Vert M_{\iota}Y\right\Vert ^{2}=\Vert M_{\iota}X_{1}\widehat{\beta}_{1}\Vert^{2}+\left\Vert \widehat{e}\right\Vert ^{2}.
\]

\end_inset


\end_layout

\begin_layout Exercise
\begin_inset CommandInset label
LatexCommand label
name "ex:e_equiv"

\end_inset

 Show that 
\begin_inset Formula $\widehat{e}$
\end_inset

 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:decomp_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

) is exactly the same as 
\begin_inset Formula $\tilde{e}$
\end_inset

 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:decomp_2"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard

\emph on
R-squared
\emph default
 is a popular measure of goodness-of-fit in the linear regression.
 The (in-sample) R-squared
\begin_inset Formula 
\[
R^{2}=\frac{\Vert M_{\iota}X_{1}\widehat{\beta}_{1}\Vert^{2}}{\left\Vert M_{\iota}Y\right\Vert ^{2}}=1-\frac{\left\Vert \tilde{e}\right\Vert ^{2}}{\left\Vert M_{\iota}Y\right\Vert ^{2}}.
\]

\end_inset

is well defined only when a constant is included in the regressors.
\end_layout

\begin_layout Exercise
Show 
\begin_inset Formula 
\[
R^{2}=\frac{\widehat{Y}'M_{\iota}\widehat{Y}}{Y'M_{\iota}Y}=\frac{\sum_{i=1}^{n}\left(\widehat{y_{i}}-\overline{y}\right)^{2}}{\sum_{i=1}^{n}\left(y_{i}-\overline{y}\right)^{2}}
\]

\end_inset

 as in the decomposition (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:decomp_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 In other words, it is the ratio between the sample variance of 
\begin_inset Formula $\widehat{Y}$
\end_inset

 and the sample variance of 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
We can decompose 
\begin_inset Formula $Y=P_{X}Y+M_{X}Y=\widehat{Y}+\widehat{e}$
\end_inset

.
 The total variation is 
\begin_inset Formula 
\[
Y'M_{\iota}Y=\left(\widehat{Y}+\widehat{e}\right)'M_{\iota}\left(\widehat{Y}+\widehat{e}\right)=\widehat{Y}'M_{\iota}\widehat{Y}+2\widehat{Y}'M_{\iota}\widehat{e}+\widehat{e}'M_{\iota}\widehat{e}=\widehat{Y}'M_{\iota}\widehat{Y}+\widehat{e}'\widehat{e}
\]

\end_inset

where the last equality follows by 
\begin_inset Formula $M_{\iota}\widehat{e}=\widehat{e}$
\end_inset

 as 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}=0$
\end_inset

 (This property fails if the regression has no constant), and 
\begin_inset Formula $\widehat{Y}'\widehat{e}=Y'P_{X}M_{X}e=0$
\end_inset

.
 
\end_layout

\end_inset

The magnitude of R-squared varies in different contexts.
 In macro models with the lagged dependent variables, it is not unusually
 to observe R-squared larger than 90%.
 In cross sectional regressions it is often below 20%.
\end_layout

\begin_layout Exercise
Consider a short regression 
\begin_inset Quotes eld
\end_inset

regress 
\begin_inset Formula $y_{i}$
\end_inset

 on 
\begin_inset Formula $x_{1i}$
\end_inset


\begin_inset Quotes erd
\end_inset

 and a long regression 
\begin_inset Quotes eld
\end_inset

regress 
\begin_inset Formula $y_{i}$
\end_inset

 on 
\begin_inset Formula $\left(x_{1i},x_{2i}\right)$
\end_inset


\begin_inset Quotes erd
\end_inset

.
 Given the same dataset 
\begin_inset Formula $\left(Y,X_{1},X_{2}\right)$
\end_inset

, show that the R-squared from the short regression is no larger than that
 from the long regression.
 In other words, we can always (weakly) increase 
\begin_inset Formula $R^{2}$
\end_inset

 by adding more regressors.
\end_layout

\begin_layout Standard
Conventionally we consider the regressions when the number of regressors
 
\begin_inset Formula $K$
\end_inset

 is much smaller the sample size 
\begin_inset Formula $n$
\end_inset

.
 In the era of big data, it can happen that we have more potential regressors
 than the sample size.
\end_layout

\begin_layout Exercise
Show 
\begin_inset Formula $R^{2}=1$
\end_inset

 when 
\begin_inset Formula $K\geq n$
\end_inset

.
 (When 
\begin_inset Formula $K>n$
\end_inset

, the matrix 
\begin_inset Formula $X'X$
\end_inset

 must be rank deficient.
 We can generalize the definition OLS fitting as any vector that minimizes
 
\begin_inset Formula $\left\Vert Y-Xb\right\Vert ^{2}$
\end_inset

 though the minimizer is not unique.
\end_layout

\begin_layout Exercise
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

n = 5; K = 6; 
\end_layout

\begin_layout Plain Layout

Y = rnorm(n)
\end_layout

\begin_layout Plain Layout

X = matrix( rnorm(n*K), n)
\end_layout

\begin_layout Plain Layout

summary( lm(Y~X) )
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
With a new dataset 
\begin_inset Formula $\left(Y^{\mathrm{new}},X^{\mathrm{new}}\right)$
\end_inset

, the 
\emph on
out-of-sample
\emph default
 (OOS) R-squared is 
\begin_inset Formula 
\[
OOS\ R^{2}=\frac{\widehat{\beta}^{\prime}X^{\mathrm{new}\prime}M_{\iota}X^{\mathrm{new}}\widehat{\beta}}{Y^{\mathrm{new}\prime}M_{\iota}Y^{\mathrm{new}}}.
\]

\end_inset

OOS R-squred measures the goodness of fit in a new dataset given the coefficient
 estimated from the original data.
 In financial market shorter-term predictive models, a person may become
 a billion if he can systematically achieve 2% OOS R-squared.
\end_layout

\begin_layout Section
Statistical Properties of Least Squares
\begin_inset CommandInset label
LatexCommand label
name "statistical-properties-of-least-squares"

\end_inset


\end_layout

\begin_layout Standard
In this section we return to the classical statistical framework under restricti
ve distributional assumption 
\begin_inset Formula 
\begin{equation}
y_{i}|x_{i}\sim N\left(x_{i}'\beta,\gamma\right)\label{eq:normal_yx}
\end{equation}

\end_inset

, where 
\begin_inset Formula $\gamma=\sigma^{2}$
\end_inset

 to ease the differentiation.
 This assumption is equivalent to 
\begin_inset Formula $e_{i}|x_{i}=\left(y_{i}-x_{i}'\beta\right)|x_{i}\sim N\left(0,\gamma\right)$
\end_inset

.
 Because the distribution of 
\begin_inset Formula $e_{i}$
\end_inset

 is invariant to 
\begin_inset Formula $x_{i}$
\end_inset

, the error term 
\begin_inset Formula $e_{i}\sim N\left(0,\gamma\right)$
\end_inset

 and is statistically independent of 
\begin_inset Formula $x_{i}$
\end_inset

.
 This is a very strong assumption.
\end_layout

\begin_layout Subsection
Maximum Likelihood Estimation
\begin_inset CommandInset label
LatexCommand label
name "maximum-likelihood-estimation"

\end_inset


\end_layout

\begin_layout Standard
The likelihood of observing a pair 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 is 
\begin_inset Formula 
\begin{align*}
f_{yx}\left(y_{i},x_{i}\right) & =f_{y|x}\left(y_{i}|x_{i}\right)f_{x}\left(x\right)\\
 & =\frac{1}{\sqrt{2\pi\gamma}}\exp\left(-\frac{1}{2\gamma}\left(y_{i}-x_{i}'\beta\right)^{2}\right)\times f_{x}\left(x\right),
\end{align*}

\end_inset

 where 
\begin_inset Formula $f_{yx}$
\end_inset

 is the joint pdf, 
\begin_inset Formula $f_{y|x}$
\end_inset

 is the conditional pdf and 
\begin_inset Formula $f_{x}$
\end_inset

 is the marginal pdf of 
\begin_inset Formula $x$
\end_inset

, and the second equality holds under the assumption (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:normal_yx"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 The likelihood a random sample 
\begin_inset Formula $\left(y_{i},x_{i}\right)_{i=1}^{n}$
\end_inset

 is 
\begin_inset Formula 
\begin{align*}
\prod_{i=1}^{n}f_{yx}\left(y_{i},x_{i}\right) & =\prod_{i=1}^{n}f_{y|x}\left(y_{i}|x_{i}\right)f_{x}\left(x\right)\\
 & =\prod_{i=1}^{n}f_{y|x}\left(y_{i}|x_{i}\right)\times\prod_{i=1}^{n}f_{x}\left(x\right)\\
 & =\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\gamma}}\exp\left(-\frac{1}{2\gamma}\left(y_{i}-x_{i}'\beta\right)^{2}\right)\times\prod_{i=1}^{n}f_{x}\left(x\right).
\end{align*}

\end_inset

The parameters of interest 
\begin_inset Formula $\left(\beta,\gamma\right)$
\end_inset

 are irrelevant to the second term 
\begin_inset Formula $\prod_{i=1}^{n}f_{x}\left(x\right)$
\end_inset

 for they appear only in the conditional likelihood 
\begin_inset Formula 
\[
\prod_{i=1}^{n}f_{y|x}\left(y_{i}|x_{i}\right)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\gamma}}\exp\left(-\frac{1}{2\gamma}\left(y_{i}-x_{i}'\beta\right)^{2}\right).
\]

\end_inset

We focus on the conditional likelihood.
 To facilitate derivation, we work with the conditional log-likelihood function
 
\begin_inset Formula 
\[
L\left(\beta,\gamma\right)=-\frac{n}{2}\log2\pi-\frac{n}{2}\log\gamma-\frac{1}{2\gamma}\sum_{i=1}^{n}\left(y_{i}-x_{i}'\beta\right)^{2},
\]

\end_inset

for 
\begin_inset Formula $\log\left(\cdot\right)$
\end_inset

 is a monotonic transformation that does not change the maximizer.
 The maximum likelihood estimator 
\begin_inset Formula $\widehat{\beta}_{MLE}$
\end_inset

 can be found using the FOC:
\begin_inset Formula 
\[
\frac{\partial}{\partial\beta}L\left(\beta,\gamma\right)=\frac{1}{2\gamma}\sum_{i=1}^{n}2x_{i}\left(y_{i}-x_{i}'\beta\right)^{2}=\frac{1}{\gamma}\sum_{i=1}^{n}x_{i}\left(y_{i}-x_{i}'\beta\right)^{2}=0.
\]

\end_inset

Rearranging the above equation in matrix form 
\begin_inset Formula $X'Y=X'X\widehat{\beta}_{MLE}$
\end_inset

, we explicitly solve 
\begin_inset Formula 
\[
\widehat{\beta}_{MLE}=(X'X)^{-1}X'Y.
\]

\end_inset

The maximum likelihood estimator (MLE) coincides with the OLS estimator.
 Similarly, the other FOC with respect to 
\begin_inset Formula $\gamma$
\end_inset

 gives 
\begin_inset Formula $\widehat{\gamma}_{\mathrm{MLE}}=\widehat{e}'\widehat{e}/n$
\end_inset

.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
revise up here Sep 23, 2020.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Classical Finite Sample Distribution
\begin_inset CommandInset label
LatexCommand label
name "finite-sample-distribution"

\end_inset


\end_layout

\begin_layout Standard
We can show the finite-sample exact distribution of 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 assuming the error term follows a Gaussian distribution.
 
\emph on
Finite sample distribution
\emph default
 means that the distribution holds for any 
\begin_inset Formula $n$
\end_inset

; it is in contrast to 
\emph on
asymptotic distribution
\emph default
, which is a large sample approximation to the finite sample distribution.
 We first review some properties of a generic jointly normal random vector.
 
\end_layout

\begin_layout Fact
\begin_inset CommandInset label
LatexCommand label
name "fact31"

\end_inset

 Let 
\begin_inset Formula $z\sim N\left(\mu,\Omega\right)$
\end_inset

 be an 
\begin_inset Formula $l\times1$
\end_inset

 random vector with a positive definite variance-covariance matrix 
\begin_inset Formula $\Omega$
\end_inset

.
 Let 
\begin_inset Formula $A$
\end_inset

 be an 
\begin_inset Formula $m\times l$
\end_inset

 non-random matrix where 
\begin_inset Formula $m\leq l$
\end_inset

.
 Then 
\begin_inset Formula $Az\sim N\left(A\mu,A\Omega A'\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Fact
\begin_inset CommandInset label
LatexCommand label
name "fact32"

\end_inset

If 
\begin_inset Formula $z\sim N\left(0,1\right)$
\end_inset

, 
\begin_inset Formula $w\sim\chi^{2}\left(d\right)$
\end_inset

 and 
\begin_inset Formula $z$
\end_inset

 and 
\begin_inset Formula $w$
\end_inset

 are independent.
 Then 
\begin_inset Formula $\frac{z}{\sqrt{w/d}}\sim t\left(d\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The OLS estimator
\begin_inset Formula 
\[
\widehat{\beta}=\left(X'X\right)^{-1}X'Y=\left(X'X\right)^{-1}X'\left(X'\beta+e\right)=\beta+\left(X'X\right)^{-1}X'e,
\]

\end_inset

and its conditional distribution can be written as 
\begin_inset Formula 
\begin{align*}
\widehat{\beta}|X & =\beta+\left(X'X\right)^{-1}X'e|X\\
 & \sim\beta+\left(X'X\right)^{-1}X'\cdot N\left(0_{n},\sigma^{2}\cdot I_{n}\right)\\
 & \sim N\left(\beta,\sigma^{2}\left(X'X\right)^{-1}X'X\left(X'X\right)^{-1}\right)\sim N\left(\beta,\sigma^{2}\left(X'X\right)^{-1}\right)
\end{align*}

\end_inset

by Fact 
\begin_inset CommandInset ref
LatexCommand ref
reference "fact31"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The 
\begin_inset Formula $k$
\end_inset

-th element of the vector coefficient
\begin_inset Formula 
\[
\widehat{\beta}_{k}|X=\eta_{k}'\widehat{\beta}|X\sim N\left(\beta_{k},\sigma^{2}\eta_{k}'\left(X'X\right)^{-1}\eta_{k}\right)\sim N\left(\beta_{k},\sigma^{2}\left(X'X\right)_{kk}^{-1}\right),
\]

\end_inset

where 
\begin_inset Formula $\eta_{k}=\left(1\left\{ l=k\right\} \right)_{l=1,\ldots,K}$
\end_inset

 is the selector of the 
\begin_inset Formula $k$
\end_inset

-th element.
\end_layout

\begin_layout Standard
In reality, 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is an unknown parameter, and 
\begin_inset Formula 
\[
s^{2}=\widehat{e}'\widehat{e}/\left(n-K\right)=e'M_{X}e/\left(n-K\right)
\]

\end_inset

is an unbiased estimator of 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 Consider the 
\begin_inset Formula $t$
\end_inset

-statistic
\begin_inset Formula 
\begin{align*}
T_{k} & =\frac{\widehat{\beta}_{k}-\beta_{k}}{\sqrt{s^{2}\left[\left(X'X\right)^{-1}\right]_{kk}}}=\frac{\widehat{\beta}_{k}-\beta_{k}}{\sqrt{\sigma^{2}\left[\left(X'X\right)^{-1}\right]_{kk}}}\cdot\frac{\sqrt{\sigma^{2}}}{\sqrt{s^{2}}}\\
 & =\frac{\left(\widehat{\beta}_{k}-\beta_{k}\right)/\sqrt{\sigma^{2}\left[\left(X'X\right)^{-1}\right]_{kk}}}{\sqrt{\frac{e'}{\sigma}M_{X}\frac{e}{\sigma}/\left(n-K\right)}}.
\end{align*}

\end_inset

The numerator follows a standard normal, and the denominator follows 
\begin_inset Formula $\frac{1}{n-K}\chi^{2}\left(n-K\right)$
\end_inset

.
 Moreover, the numerator and the denominator are statistically independent
 (Basu's theorem).
 As a result, we conclude 
\begin_inset Formula $T_{k}\sim t\left(n-K\right)$
\end_inset

 by Fact 
\begin_inset CommandInset ref
LatexCommand ref
reference "fact32"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 This finite sample distribution allows us to conduct statistical inference.
\end_layout

\begin_layout Subsection
Mean and Variance
\begin_inset CommandInset label
LatexCommand label
name "mean-and-variance"

\end_inset


\end_layout

\begin_layout Standard
Now we relax the normality assumption and statistical independence.
 Instead, we represent the regression model as 
\begin_inset Formula $Y=X\beta+e$
\end_inset

 and 
\begin_inset Formula 
\begin{align*}
E[e|X] & =0_{n}\\
\mathrm{var}\left[e|X\right] & =E\left[ee'|X\right]=\sigma^{2}I_{n}.
\end{align*}

\end_inset

where the first condition is the 
\emph on
mean independence
\emph default
 assumption, and the second condition is the 
\emph on
homoskedasticity
\emph default
 assumption.
 These assumptions are about the first and second 
\emph on
moments
\emph default
 of 
\begin_inset Formula $e_{i}$
\end_inset

 conditional on 
\begin_inset Formula $x_{i}$
\end_inset

.
 Unlike the normality assumption, they do not restrict the distribution
 of 
\begin_inset Formula $e_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
Unbiasedness: 
\begin_inset Formula 
\begin{align*}
E\left[\widehat{\beta}|X\right] & =E\left[\left(X'X\right)^{-1}XY|X\right]=E\left[\left(X'X\right)^{-1}X\left(X'\beta+e\right)|X\right]\\
 & =\beta+\left(X'X\right)^{-1}XE\left[e|X\right]=\beta.
\end{align*}

\end_inset

Unbiasedness does not rely on homoskedasticity.
\end_layout

\begin_layout Itemize
Variance: 
\begin_inset Formula 
\[
\begin{aligned}\mathrm{var}\left[\widehat{\beta}|X\right] & =E\left[\left(\widehat{\beta}-E\widehat{\beta}\right)\left(\widehat{\beta}-E\widehat{\beta}\right)'|X\right]\\
 & =E\left[\left(\widehat{\beta}-\beta\right)\left(\widehat{\beta}-\beta\right)'|X\right]\\
 & =E\left[\left(X'X\right)^{-1}X'ee'X\left(X'X\right)^{-1}|X\right]\\
 & =\left(X'X\right)^{-1}X'E\left[ee'|X\right]X\left(X'X\right)^{-1}
\end{aligned}
\]

\end_inset

where the second equality holds as 
\begin_inset Formula $E\left[\widehat{\beta}\right]=E\left[E\left[\widehat{\beta}|X\right]\right]=\beta$
\end_inset

.
 Under the assumption of homoskedasticity, it can be simplified as
\begin_inset Formula 
\[
\begin{aligned}\mathrm{var}\left[\widehat{\beta}|X\right] & =\left(X'X\right)^{-1}X'\left(\sigma^{2}I_{n}\right)X\left(X'X\right)^{-1}=\sigma^{2}\left(X'X\right)^{-1}.\end{aligned}
\]

\end_inset


\end_layout

\begin_layout Example
(Heteroskedasticity) If 
\begin_inset Formula $e_{i}=x_{i}u_{i}$
\end_inset

, where 
\begin_inset Formula $x_{i}$
\end_inset

 is a scalar random variable, 
\begin_inset Formula $u_{i}$
\end_inset

 is statistically independent of 
\begin_inset Formula $x_{i}$
\end_inset

, 
\begin_inset Formula $E\left[u_{i}\right]=0$
\end_inset

 and 
\begin_inset Formula $E\left[u_{i}^{2}\right]=\sigma^{2}$
\end_inset

.
 Then 
\begin_inset Formula $E\left[e_{i}|x_{i}\right]=0$
\end_inset

 but 
\begin_inset Formula $E\left[e_{i}^{2}|x_{i}\right]=\sigma^{2}x_{i}^{2}$
\end_inset

 is a function of 
\begin_inset Formula $x_{i}$
\end_inset

.
 We say 
\begin_inset Formula $e_{i}^{2}$
\end_inset

 is a heteroskedastic error.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Plain Layout
Notice that the above derivation only applies when 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 are independent across 
\begin_inset Formula $i$
\end_inset

.
 In time series the conditional zero mean condition (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:e_mean_zero"
plural "false"
caps "false"
noprefix "false"

\end_inset

) has to be strengthened to 
\begin_inset Formula $E\left[e_{i}|x_{1},\ldots,x_{n}\right]=0$
\end_inset

 for unbiasedness.
\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<custom-dev, echo=FALSE>>= 
\end_layout

\begin_layout Plain Layout

my_pdf = function(file, width, height) {
\end_layout

\begin_layout Plain Layout

  pdf(file, width = 4, height = 4) 
\end_layout

\begin_layout Plain Layout

} 
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<dev-demo, dev='my_pdf', fig.ext='pdf'>>=
\end_layout

\begin_layout Plain Layout

n = 100; X = rnorm(n)
\end_layout

\begin_layout Plain Layout

e1 = rnorm(n); 
\end_layout

\begin_layout Plain Layout

plot( y = e1, x = X, col = 
\begin_inset Quotes eld
\end_inset

blue
\begin_inset Quotes erd
\end_inset

, ylab = 
\begin_inset Quotes eld
\end_inset

e
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Plain Layout

e2 = X * rnorm(n); 
\end_layout

\begin_layout Plain Layout

points( y = e2, x = X, col = 
\begin_inset Quotes eld
\end_inset

red
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Example
It is important to notice that independently and identically distributed
 sample (iid) 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 does not imply homoskedasticity.
 Homoskedasticity or heterskdasticity is about the relationship between
 
\begin_inset Formula $\left(x_{i},e_{i}=y_{i}-\beta x\right)$
\end_inset

, whereas iid is about the relationship between 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 and 
\begin_inset Formula $\left(y_{j},x_{j}\right)$
\end_inset

 for 
\begin_inset Formula $i\neq j$
\end_inset

.
\end_layout

\begin_layout Subsection
Gauss-Markov Theorem
\end_layout

\begin_layout Standard
Gauss-Markov theorem is concerned about the optimality of OLS.
 It justifies OLS as the efficient estimator among all linear unbiased ones.
 
\emph on
Efficient
\emph default
 here means that it enjoys the smallest variance in a family of estimators.
\end_layout

\begin_layout Standard
We have shown that OLS is unbiased in that 
\begin_inset Formula $E\left[\widehat{\beta}\right]=\beta$
\end_inset

.
 There are numerous linearly unbiased estimators.
 For example, 
\begin_inset Formula $\left(Z'X\right)^{-1}Z'y$
\end_inset

 for 
\begin_inset Formula $z_{i}=x_{i}^{2}$
\end_inset

 is unbiased because 
\begin_inset Formula $E\left[\left(Z'X\right)^{-1}Z'y\right]=E\left[\left(Z'X\right)^{-1}Z'\left(X\beta+e\right)\right]=\beta$
\end_inset

.
 We cannot say OLS is better than those other unbiased estimators because
 they are equally good in this aspect.
 Thus, we move to the second order property of variance: an estimator is
 better if its variance is smaller.
\end_layout

\begin_layout Fact
For two generic random vectors 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 of the same size, we say 
\begin_inset Formula $X$
\end_inset

's variance is smaller or equal to 
\begin_inset Formula $Y$
\end_inset

's variance if 
\begin_inset Formula $\left(\Omega_{Y}-\Omega_{X}\right)$
\end_inset

 is a positive semi-definite matrix.
 The comparison is defined this way because for any non-zero constant vector
 
\begin_inset Formula $c$
\end_inset

, the variance of the linear combination of 
\begin_inset Formula $X$
\end_inset


\begin_inset Formula 
\[
\mathrm{var}\left(c'X\right)=c'\Omega_{X}c\leq c'\Omega_{Y}c=\mathrm{var}\left(c'Y\right)
\]

\end_inset

is no bigger than the same linear combination of 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\tilde{\beta}=A'y$
\end_inset

 be a generic linear estimator, where 
\begin_inset Formula $A$
\end_inset

 is any 
\begin_inset Formula $n\times K$
\end_inset

 functions of 
\begin_inset Formula $X$
\end_inset

.
 As 
\begin_inset Formula 
\[
E\left[A'y|X\right]=E\left[A'\left(X\beta+e\right)|X\right]=A'X\beta.
\]

\end_inset

So the linearity and unbiasedness of 
\begin_inset Formula $\tilde{\beta}$
\end_inset

 implies 
\begin_inset Formula $A'X=I_{n}$
\end_inset

.
 Moreover, the variance 
\begin_inset Formula 
\[
\mbox{var}\left(A'y|X\right)=E\left[\left(A'y-\beta\right)\left(A'y-\beta\right)'|X\right]=E\left[A'ee'A|X\right]=\sigma^{2}A'A.
\]

\end_inset

Let 
\begin_inset Formula $C=A-X\left(X'X\right)^{-1}.$
\end_inset

 
\begin_inset Formula 
\[
\begin{aligned}A'A-\left(X'X\right)^{-1} & =\left(C+X\left(X'X\right)^{-1}\right)'\left(C+X\left(X'X\right)^{-1}\right)-\left(X'X\right)^{-1}\\
 & =C'C+\left(X'X\right)^{-1}X'C+C'X\left(X'X\right)^{-1}\\
 & =C'C,
\end{aligned}
\]

\end_inset

where the last equality follows as 
\begin_inset Formula 
\[
\left(X'X\right)^{-1}X'C=\left(X'X\right)^{-1}X'\left(A-X\left(X'X\right)^{-1}\right)=\left(X'X\right)^{-1}-\left(X'X\right)^{-1}=0.
\]

\end_inset

Therefore 
\begin_inset Formula $A'A-\left(X'X\right)^{-1}$
\end_inset

 is a positive semi-definite matrix.
 The variance of any 
\begin_inset Formula $\tilde{\beta}$
\end_inset

 is no smaller than the OLS estimator 
\begin_inset Formula $\widehat{\beta}$
\end_inset

.
 The above derivation shows OLS achieves the smallest variance among all
 linear unbiased estimators.
\end_layout

\begin_layout Standard
Homoskedasticity is a restrictive assumption.
 Under homoskedasticity, 
\begin_inset Formula $\mathrm{var}\left[\widehat{\beta}\right]=\sigma^{2}\left(X'X\right)^{-1}$
\end_inset

.
 Popular estimator of 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is the sample mean of the residuals 
\begin_inset Formula $\widehat{\sigma}^{2}=\frac{1}{n}\widehat{e}'\widehat{e}$
\end_inset

 or the unbiased one 
\begin_inset Formula $s^{2}=\frac{1}{n-K}\widehat{e}'\widehat{e}$
\end_inset

.
 Under heteroskedasticity, Gauss-Markov theorem does not apply.
\end_layout

\begin_layout Section
Summary
\end_layout

\begin_layout Standard
The linear algebraic properties holds in finite sample no matter the data
 are taken as fixed numbers or random variables.
 The exact distribution under the normality assumption of the error term
 is the classical statistical results.
 The Gauss Markov theorem holds under two crucial assumptions: linear CEF
 and homoskedasticity.
 
\end_layout

\begin_layout Standard

\series bold
Historical notes
\series default
: Carl Friedrich Gauss (1777â€“1855) claimed he had come up with the operation
 of OLS in 1795.
 With only three data points at hand, Gauss successfully applied his method
 to predict the location of the dwarf planet Ceres in 1801.
 While Gauss did not publish the work on OLS until 1809, Adrien-Marie Legendre
 (1752â€“1833) presented this method in 1805.
 Today people tend to attribute OLS to Gauss, assuming that a giant like
 Gauss had no need to tell a lie to steal Legendre's discovery.
 
\end_layout

\begin_layout Standard
MLE was promulgated and popularized by Ronald Fisher (1890â€“1962).
 He was a major contributor of the frequentist approach which dominates
 mathematical statistics today, and he sharply criticized the Bayesian approach.
 Fisher collected the iris flower dataset of 150 observations in his biological
 study in 1936, which can be displayed in R by typing 
\family typewriter
iris
\family default
.
 
\end_layout

\begin_layout Section
Appendix
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $A$
\end_inset

 be any 
\begin_inset Formula $n\times K$
\end_inset

 generic real matrix.
 
\emph on
Singular value decomposition
\emph default
 (SVD) factorizes 
\begin_inset Formula $A=USV'$
\end_inset

, where 
\begin_inset Formula $U$
\end_inset

 is an 
\begin_inset Formula $n\times n$
\end_inset

 real unitary matrix (A real unitary matrix is invertible and 
\begin_inset Formula $U'U=UU'=I$
\end_inset

, which implies 
\begin_inset Formula $U^{-1}=U'$
\end_inset

), 
\begin_inset Formula $S=\begin{bmatrix}S_{1}\\
0_{\left(n-K\right)\times K}
\end{bmatrix}$
\end_inset

 is an 
\begin_inset Formula $n\times K$
\end_inset

 rectangular diagonal matrix with 
\begin_inset Formula $S_{1}$
\end_inset

 a 
\begin_inset Formula $K\times K$
\end_inset

 diagonal matrix of non-negative real elements (called 
\emph on
singular values
\emph default
), and 
\begin_inset Formula $V$
\end_inset

 is a 
\begin_inset Formula $K\times K$
\end_inset

 real unitary matrix.
\end_layout

\begin_layout Standard
We apply SVD to the projection matrix 
\begin_inset Formula $P_{X}=X\left(X'X\right)^{-1}X$
\end_inset

, where 
\begin_inset Formula $X$
\end_inset

 is an 
\begin_inset Formula $n\times K$
\end_inset

 data matrix with 
\begin_inset Formula $K$
\end_inset

 linearly independent columns.
 Substitute 
\begin_inset Formula $X=USV'$
\end_inset

 into 
\begin_inset Formula $P_{X}$
\end_inset

:
\begin_inset Formula 
\begin{align*}
P_{X} & =USV'\left(VS'U'USV'\right)^{-1}VS'U'=USV'\left(VS'SV'\right)^{-1}VS'U'\\
 & =USV'V^{\prime-1}\left(S'S\right)^{-1}V^{-1}VS'U'=US\left(S'S\right)^{-1}S'U'\\
 & =U\begin{bmatrix}S_{1}\\
0
\end{bmatrix}S_{1}^{-1}S_{1}^{-1}\begin{bmatrix}S_{1} & 0\end{bmatrix}U'=U\begin{bmatrix}I_{K} & 0_{K\times(n-K)}\\
0_{(n-K)\times K} & 0_{(n-K)\times(n-K)}
\end{bmatrix}U'\\
 & =U\,\mathrm{diag}\left(\iota_{K},0_{n-K}\right)U'.
\end{align*}

\end_inset

 All real symmetric matrices are diagonalizable, and the the last expression
 is the diagonalization of 
\begin_inset Formula $P_{X}$
\end_inset

.
 The projection matrix 
\begin_inset Formula $P_{X}$
\end_inset

 has 
\begin_inset Formula $K$
\end_inset

 repeated eigenvalues of 1 and 
\begin_inset Formula $\left(n-K\right)$
\end_inset

 repeated eigenvalues of 0, and obviously 
\begin_inset Formula $\mathrm{rank}(P_{X})=K$
\end_inset

.
 
\end_layout

\begin_layout Standard
Two generic square matrices 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 are 
\emph on
similar
\emph default
 if there exists an invertible matrix 
\begin_inset Formula $Q$
\end_inset

 such that 
\begin_inset Formula $A=Q^{-1}BQ$
\end_inset

.
 By this definition, 
\begin_inset Formula $P_{X}$
\end_inset

 is similar to the diagonal matrix 
\begin_inset Formula $\mathrm{diag}\left(\iota_{K},0_{n-K}\right)$
\end_inset

, and 
\begin_inset Formula $M_{X}=I_{n}-P_{X}$
\end_inset

 is similar to 
\begin_inset Formula $\mathrm{diag}\left(0_{K},\iota_{n-K}\right)$
\end_inset

 because 
\begin_inset Formula 
\begin{align*}
U'\,M_{X}U & =U'\,\left(I_{n}-P_{X}\right)U=U'U-U'\,P_{X}U\\
 & =I_{n}-\mathrm{diag}\left(\iota_{K},0_{n-K}\right)=\mathrm{diag}\left(0_{K},\iota_{n-K}\right).
\end{align*}

\end_inset

It implies that 
\begin_inset Formula $\mathrm{rank}(M_{X})=n-K$
\end_inset

.
\end_layout

\begin_layout Standard
Both 
\begin_inset Formula $P_{X}$
\end_inset

 and 
\begin_inset Formula $M_{X}$
\end_inset

 are symmetric idempotent matrices.
 For a general idempotent matrices 
\begin_inset Formula $C$
\end_inset

 which does not have to be symmetric,
\end_layout

\begin_layout Itemize
\begin_inset Formula $C$
\end_inset

 is diagonalizable (See 
\begin_inset CommandInset citation
LatexCommand cite
after "p.148"
key "horn2012matrix"
literal "false"

\end_inset

).
\end_layout

\begin_layout Standard
This fact immediately implies that
\end_layout

\begin_layout Itemize
All eigenvalues of 
\begin_inset Formula $C$
\end_inset

 are either 0 and 1;
\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathrm{rank}\left(C\right)=\mathrm{trace}\left(C\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bigskip
\end_layout

\begin_layout Plain Layout


\backslash
texttt{ Zhentao Shi.
 Sep 26.}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "ref_teaching"
options "chicagoa"

\end_inset


\end_layout

\end_body
\end_document
